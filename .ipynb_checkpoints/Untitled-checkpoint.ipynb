{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef11e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "labels = []\n",
    "with open(r\"E:\\Python test Work\\ConAno\\labels\\UCF_test.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        if \"Normal\" in line.strip():\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5649a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import torch.optim\n",
    "import os\n",
    "from utils.utils import *\n",
    "from utils.evaluation import *\n",
    "from utils.model_utils import *\n",
    "import numpy as np\n",
    "from config import CFG\n",
    "from torch.utils.data import DataLoader\n",
    "from models.modules import PositionalEncoding1D\n",
    "from Loss.losses import *\n",
    "from dataset import MyDataset\n",
    "import warnings\n",
    "from utils.evaluation import evaluate_result\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "log_theta = torch.nn.LogSigmoid()\n",
    "\n",
    "\n",
    "def get_dataloader(args: CFG):\n",
    "    data_train = torch.from_numpy(np.load(args.train_path)).reshape(-1, args.snippets, 1024)\n",
    "\n",
    "    label_train = torch.load(args.label_train_path)\n",
    "    dataset_train = MyDataset(data_train, label_train)\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=CFG.Batch_size, shuffle=True)\n",
    "\n",
    "    data_test = torch.from_numpy(np.load(args.test_path)).reshape(-1, args.snippets, 1024)\n",
    "\n",
    "    label_test = None\n",
    "    dataset_test = MyDataset(data_test, label_test, mode='test')\n",
    "    test_loader = DataLoader(dataset_test, batch_size=CFG.Batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train_meta_epoch(args: CFG, epoch, trainloader, normalizing_flow, optimizer, POS_EMB: PositionalEncoding1D,\n",
    "                     metric_recoder: MetricRecoder):\n",
    "    \"\"\"\n",
    "    :param args:\n",
    "    :param epoch:\n",
    "    :param trainloader: [Batch_size , 32,1024]\n",
    "    :param normalizing_flow:\n",
    "    :param optimizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    normalizing_flow.to(args.device)\n",
    "    normalizing_flow.train()\n",
    "    adjust_learning_rate(args, optimizer, epoch)\n",
    "    I = len(trainloader)\n",
    "    for sub_epoch in range(args.sub_epochs):\n",
    "        total_loss, loss_count = 0.0, 0\n",
    "        logps_list = []\n",
    "        for (i, loader) in enumerate(trainloader):\n",
    "            # lr = warmup_learning_rate(args, epoch, i + sub_epoch * I, I * args.sub_epochs, optimizer)\n",
    "            loader = loader.to(args.device)  # [4,32,1024]\n",
    "\n",
    "            m_b = torch.hstack([torch.zeros(loader.shape[1] // 2), torch.ones(loader.shape[1] // 2)]).unsqueeze(\n",
    "                0).repeat(args.Batch_size, 1).to(args.device)  # [4,32]\n",
    "\n",
    "            if epoch == 0 or sub_epoch < args.normal_sub_epoch:  # Normal only\n",
    "                loader = loader[:, :args.snippets, :]\n",
    "                m_b = m_b[:, :args.snippets]\n",
    "            b, n, c = loader.shape\n",
    "\n",
    "            loader = loader.reshape(-1, c)  # [Batch_size * 16(32) , 1024]\n",
    "            m_b = m_b.reshape(-1)\n",
    "            pos_embed = POS_EMB(loader.reshape(b, n, c)).reshape(-1, args.pos_embed_dim).to(\n",
    "                args.device)  # [B,16(32),1024]\n",
    "\n",
    "            perm = torch.randperm(b * n).to(args.device)\n",
    "            e_b = loader[perm]\n",
    "            m_b = m_b[perm]\n",
    "            p_b = pos_embed[perm]\n",
    "\n",
    "            if args.flow_arch == 'flow_model':\n",
    "                z, log_jac_det = normalizing_flow(e_b)  # [4*16,1024] , [4*16]\n",
    "            else:\n",
    "                z, log_jac_det = normalizing_flow(e_b, [p_b, ])\n",
    "\n",
    "            if epoch == 0:\n",
    "                logps = get_logp(c, z, log_jac_det) / c  # [4*16]\n",
    "\n",
    "                loss = -log_theta(logps).mean()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                loss_count += 1\n",
    "            else:\n",
    "                if sub_epoch < args.normal_sub_epoch:\n",
    "                    logps = get_logp(c, z, log_jac_det)  # Batch_size * 16\n",
    "                    logps = logps / c\n",
    "                    if args.focal_weighting:\n",
    "                        normal_weights = normal_fl_weighting(logps.detach())\n",
    "\n",
    "                        loss = -log_theta(logps) * normal_weights\n",
    "                        loss = loss.mean()\n",
    "\n",
    "                    else:\n",
    "                        loss = -log_theta(logps).mean()\n",
    "                else:\n",
    "                    logps = get_logp(c, z, log_jac_det)\n",
    "\n",
    "                    logps = logps / c\n",
    "                    if args.focal_weighting:\n",
    "                        logps_detach = logps.detach()\n",
    "                        normal_logps = logps_detach[m_b == 0]\n",
    "                        anomaly_logps = logps_detach[m_b == 1]\n",
    "                        nor_weights = normal_fl_weighting(normal_logps)\n",
    "                        ano_weights = abnormal_fl_weighting(anomaly_logps)\n",
    "                        weights = nor_weights.new_zeros(logps_detach.shape)\n",
    "                        weights[m_b == 0] = nor_weights\n",
    "                        weights[m_b == 1] = ano_weights\n",
    "                        loss_ml = -log_theta(logps[m_b == 0]) * nor_weights  # (256, )\n",
    "                        loss_ml = torch.mean(loss_ml)\n",
    "                    else:\n",
    "\n",
    "                        loss_ml = -log_theta(logps[m_b == 0])\n",
    "                        loss_ml = torch.mean(loss_ml)\n",
    "\n",
    "                    boundaries = get_logp_boundary(logps, m_b, args.pos_beta, args.margin_abnormal_negative,\n",
    "                                                   args.margin_abnormal_positive, args.normalizer)\n",
    "                    # print(boundaries)  # b_n,b_a_negative,b_a_positive\n",
    "\n",
    "                    if args.focal_weighting:\n",
    "                        loss_n_con, loss_a_con_pos = calculate_bg_spp_loss(logps, m_b, boundaries,\n",
    "                                                                           args.normalizer,\n",
    "                                                                           weights=weights, mode=2)\n",
    "                    else:\n",
    "                        loss_n_con, loss_a_con_pos = calculate_bg_spp_loss(logps, m_b, boundaries,\n",
    "                                                                           args.normalizer, mode=2)\n",
    "                    # print(f\"Loss_ml {loss_ml}, loss_n_con {loss_n_con}, loss_a_con {loss_a_con_pos}\")\n",
    "                    loss = loss_ml + args.bgspp_lambda * (loss_n_con + loss_a_con_pos)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_item = loss.item()\n",
    "                if math.isnan(loss_item):\n",
    "                    total_loss += 0.0\n",
    "                    loss_count += 0\n",
    "                else:\n",
    "                    total_loss += loss.item()\n",
    "                    loss_count += 1\n",
    "            logps_list.append(logps)\n",
    "        # print(f\"Debug {total_loss} {loss_count}\")\n",
    "        metric_recoder.update(epoch=epoch, sub_epoch=sub_epoch, loss=total_loss / loss_count if loss_count != 0 else -1)\n",
    "        print(metric_recoder)\n",
    "    return logps_list\n",
    "\n",
    "\n",
    "def validate(args: CFG, epoch, data_loader, normalizing_flow, POS_EMBED, metric_recoder: MetricRecoder):\n",
    "    print(\"Compute loss and scores\")\n",
    "    normalizing_flow = normalizing_flow.eval()\n",
    "    total_loss, loss_count = 0.0, 0\n",
    "    logps_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, feature in enumerate(data_loader):\n",
    "            b, n, dim = feature.shape\n",
    "            feature = feature.to(args.device).reshape(-1, 1024)\n",
    "            pos_embed = POS_EMBED(feature.reshape(b, n, dim)).reshape(-1, args.pos_embed_dim).to(args.device)\n",
    "            if args.flow_arch == 'flow_model':\n",
    "                z, log_jac_det = normalizing_flow(feature)\n",
    "            else:\n",
    "                z, log_jac_det = normalizing_flow(feature, [pos_embed, ])\n",
    "            logps = get_logp(dim, z, log_jac_det)\n",
    "\n",
    "            logps = logps / dim\n",
    "            loss = -log_theta(logps).mean()\n",
    "            total_loss += loss.item()\n",
    "            loss_count += 1\n",
    "            logps_list.append(logps.reshape(b, n))\n",
    "\n",
    "        mean_loss = total_loss / loss_count\n",
    "        scores = convert_to_anomaly_scores(args, logps_list).detach().cpu().numpy()\n",
    "\n",
    "        roc_auc = evaluate_result(scores, args.label_test_path)\n",
    "        metric_recoder.update(loss=mean_loss, roc_auc=roc_auc, epoch=epoch)\n",
    "        print(metric_recoder)\n",
    "    return mean_loss, roc_auc,logps_list\n",
    "\n",
    "\n",
    "def train(args: CFG):\n",
    "    trainloader, test_loader = get_dataloader(args)  # Correct\n",
    "\n",
    "    normalizing_flow = get_flow_model(args, 1024)  # Correct\n",
    "    optimizer = torch.optim.Adam(normalizing_flow.parameters(), lr=args.lr)  # Correct\n",
    "    pos_embed = PositionalEncoding1D(args.pos_embed_dim)  # Correct\n",
    "    train_recoder = MetricRecoder(mode='train')  # Correct\n",
    "    test_recoder = MetricRecoder(mode='test')  # Correct\n",
    "    for epoch in range(args.num_epochs):\n",
    "        logps_list = train_meta_epoch(args, epoch, trainloader, normalizing_flow, optimizer, pos_embed,\n",
    "                                      metric_recoder=train_recoder)\n",
    "        # if epoch == 1:\n",
    "        #     logps_list = torch.concat(logps_list, dim=0).detach().cpu().numpy()\n",
    "        #     print(logps_list.shape)\n",
    "        #     exit()\n",
    "        #     sns.histplot(logps_list, kde=True)\n",
    "        #     plt.show()\n",
    "        validate(args, epoch, test_loader, normalizing_flow, pos_embed, test_recoder)\n",
    "        # exit()\n",
    "    if args.save_result:\n",
    "        torch.save(normalizing_flow.state_dict(), os.path.join(args.log_path, 'model.pt'))\n",
    "        with open(os.path.join(args.log_path, 'train.pickle'), \"wb\") as f:\n",
    "            pickle.dump(train_recoder, f)\n",
    "\n",
    "        with open(os.path.join(args.log_path, 'test.pickle'), \"wb\") as f:\n",
    "            pickle.dump(test_recoder, f)\n",
    "\n",
    "\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "#\n",
    "def check_result(args, model, checkpoint):\n",
    "    model.load_state_dict(torch.load(checkpoint))\n",
    "    model.to(args.device)\n",
    "    model.eval()\n",
    "    _, test_loader = get_dataloader(args)\n",
    "    pos_embed = PositionalEncoding1D(args.pos_embed_dim)\n",
    "    test_recoder = MetricRecoder(mode='test')\n",
    "    _, _, res = validate(args, 0, test_loader, model, pos_embed, test_recoder)\n",
    "\n",
    "    scores = convert_to_anomaly_scores(args, res).detach().cpu().numpy()\n",
    "#     evaluate_result(scores, args.label_test_path)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# train_loader, test_loader = get_dataloader(args)\n",
    "# print(len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47019aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Normalizing Flow => Feature Dimension:  1024\n",
      "Conditional Normalizing Flow => Feature Dimension:  1024\n"
     ]
    }
   ],
   "source": [
    "args = CFG()\n",
    "model = get_flow_model(args, 1024)\n",
    "model2 = get_flow_model(args,1024)\n",
    "\n",
    "# score = check_result(args, model, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1b362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = os.path.join(args.log_path, 'model.pt')\n",
    "checkpoint2 = os.path.join(args.log_path,'model_constrastive_new.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10adc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1281,16,1024 => -1,1024 + Pos_embedding => Model => Z (-1,1024) => Normal Distribution\n",
    "\n",
    "def get_score_two_models(model,model2,checkpoint1,checkpoint2,data_path):\n",
    "    model.load_state_dict(torch.load(checkpoint1))\n",
    "    model.train()\n",
    "    model2.load_state_dict(torch.load(checkpoint2))\n",
    "    model2.train()\n",
    "    model = model.to('cuda')\n",
    "    model2 = model2.to('cuda')\n",
    "    data = torch.from_numpy(np.load(data_path)).reshape(-1,32,1024)\n",
    "    data_loader = DataLoader(data,batch_size = 16,shuffle = False)\n",
    "    with torch.no_grad():\n",
    "        output = []\n",
    "        output2 = []\n",
    "        output_det = []\n",
    "        output_det2 = []\n",
    "        for d in data_loader:\n",
    "            inp = d.to('cuda').reshape(-1,1024)\n",
    " \n",
    "            POS = PositionalEncoding1D(args.pos_embed_dim).to('cuda')\n",
    "            pos = POS(d).reshape(-1,args.pos_embed_dim).to('cuda')\n",
    "            \n",
    "            out = model(inp,[pos,])\n",
    "            out2 = model2(inp,[pos,])\n",
    "            output.append(out[0].reshape(-1,32,1024))\n",
    "            output2.append(out2[0].reshape(-1,32,1024))\n",
    "            output_det.append(out[1].reshape(-1,32))\n",
    "            output_det2.append(out2[1].reshape(-1,32))\n",
    "\n",
    "    output = torch.concat(output,dim=0).reshape(-1,1024)\n",
    "    output_det = torch.concat(output_det,dim =0).reshape(-1)\n",
    "    output2 = torch.concat(output2,dim=0).reshape(-1,1024)    \n",
    "    output_det2 = torch.concat(output_det2,dim =0 ).reshape(-1)\n",
    "    \n",
    "    score = convert_to_anomaly_scores(args,get_logp(1024,output,output_det) / 1024)\n",
    "    score2 = convert_to_anomaly_scores(args,get_logp(1024,output2,output_det2) / 1024)\n",
    "    \n",
    "    return score,score2,get_logp(1024,output,output_det) / 1024,get_logp(1024,output2,output_det2) / 1024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5dec38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score,score2,output,output2 = get_score_two_models(model,model2,checkpoint,checkpoint2,r'E:/2023/NaverProject/LastCodingProject/Binary_file/X_test_flow.npy')\n",
    "# score_train,score2_train,logp,logp2 = get_score_two_models(model,model2,checkpoint,checkpoint2,r'E:/2023/NaverProject/LastCodingProject/Binary_file/X_train_flow.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a4d92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_training(args:CFG,model_constrastive):\n",
    "    train_loader,_ = get_dataloader(args)\n",
    "    model_constrastive.train()\n",
    "    model_constrastive.to('cuda')\n",
    "    logp_list = []\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inp = data.reshape(-1,1024).to('cuda')\n",
    "        POS = PositionalEncoding1D(args.pos_embed_dim)\n",
    "        pos = POS(data).to('cuda').reshape(-1,args.pos_embed_dim)\n",
    "        with torch.no_grad():\n",
    "            z,logdet = model_constrastive(inp,[pos])\n",
    "\n",
    "        logp = get_logp(1024,z,logdet) / 1024\n",
    "        label = torch.hstack([torch.zeros(data.shape[1] // 2), torch.ones(data.shape[1] // 2)]).unsqueeze(\n",
    "                0).repeat(args.Batch_size, 1).to(args.device).reshape(-1)  # [4,32]\n",
    "        logp_list.append(logp.reshape(-1,64))\n",
    "    return logp_list\n",
    "        \n",
    "args = CFG()\n",
    "model.load_state_dict(torch.load(checkpoint2))\n",
    "# logp_list = validate_training(args,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbf8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logps_tensor = torch.concat(logp_list,dim = 0).reshape(-1)\n",
    "# label = torch.hstack([torch.zeros(32), torch.ones(32)]).unsqueeze(\n",
    "#         0).repeat(810, 1).reshape(-1)\n",
    "\n",
    "def save_visualization2(logps, labels, name_fig):\n",
    "    \"\"\"\n",
    "    :param logps: [Batch_size * 32]\n",
    "    :param labels: [Batch_size * 32]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logp_normal = logps[labels == 0]\n",
    "    logp_abnormal = logps[labels != 0]\n",
    "    print(logp_normal.shape,logp_abnormal.shape)\n",
    "    n_idx = int(len(logp_normal) * 0.4)\n",
    "    sorted_indices = torch.sort(logp_normal)[1]\n",
    "    n_idx = sorted_indices[n_idx]\n",
    "    b_n = logp_normal[n_idx]\n",
    "    plt.figure()\n",
    "    sns.distplot(logp_normal.detach().cpu().numpy(), label='normal')\n",
    "    sns.distplot(logp_abnormal.detach().cpu().numpy(), label='abnormal')\n",
    "    plt.axvline(b_n.cpu().item(), color='red', linestyle='--')\n",
    "    plt.legend()\n",
    "    if name_fig is not None:\n",
    "        plt.savefig(name_fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# save_visualization2(logps_tensor,label,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a47f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87b681ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postpress(curve, seg_size=32):\n",
    "    leng = curve.shape[0]\n",
    "    window_size = leng // seg_size\n",
    "    new_curve = np.zeros_like(curve)\n",
    "    for i in range(seg_size):\n",
    "        new_curve[window_size * i:window_size * (i + 1)] = np.mean(curve[window_size * i:window_size * (i + 1)])\n",
    "    if leng > window_size * seg_size:\n",
    "        new_curve[seg_size * window_size:] = np.mean(curve[seg_size * window_size:])\n",
    "    return new_curve\n",
    "\n",
    "\n",
    "def evaluate_result_tmp(score, label_path):\n",
    "    videos = {}\n",
    "    with open(label_path, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            video_len = int(line.strip().split(' ')[1])\n",
    "            sub_video_gt = np.zeros((video_len,), dtype=np.int8)\n",
    "            anomaly_tuple = line.split(' ')[3:]\n",
    "            for ind in range(len(anomaly_tuple) // 2):\n",
    "                start = int(anomaly_tuple[2 * ind])\n",
    "                end = int(anomaly_tuple[2 * ind + 1])\n",
    "                if start > 0:\n",
    "                    sub_video_gt[start:end] = 1\n",
    "            videos[idx] = sub_video_gt\n",
    "\n",
    "    GT = []\n",
    "    ANS = []\n",
    "\n",
    "    GT_matrix = []\n",
    "    ANS_matrix = []\n",
    "\n",
    "    for vid in videos:\n",
    "        cur_ab = score[vid]\n",
    "        cur_gt = videos[vid]\n",
    "        ratio = float(len(cur_gt)) / float(len(cur_ab))\n",
    "        cur_ans = np.zeros_like(cur_gt, dtype='float32')\n",
    "        for i in range(len(cur_ab)):\n",
    "            b = int(i * ratio + 0.5)\n",
    "            e = int((i + 1) * ratio + 0.5)\n",
    "            cur_ans[b: e] = cur_ab[i]\n",
    "        cur_ans = postpress(cur_ans, seg_size=64)\n",
    "        GT_matrix.append(cur_gt.tolist())\n",
    "        ANS_matrix.append(cur_ans.tolist())\n",
    "        GT.extend(cur_gt.tolist())\n",
    "        ANS.extend(cur_ans.tolist())\n",
    "    # for i, (gt, ans) in enumerate(tqdm(zip(GT_matrix, ANS_matrix))):\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(gt, color='blue')\n",
    "    #     plt.plot(ans, color='red')\n",
    "    #     plt.savefig(os.path.join('Result', f\"{i}.png\"))\n",
    "    #     plt.close()\n",
    "    return roc_auc_score(GT, ANS),GT_matrix,ANS_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358005e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_logp_normal(logp,label_train):\n",
    "    if logp.dim() == 1:\n",
    "        logp = logp.reshape(-1,32)\n",
    "    logp_normal = logp[label_train == 0]\n",
    "    logp_abnormal = logp[label_train != 0]\n",
    "    logp_normal = logp_normal.reshape(-1).detach().cpu()\n",
    "    logp_abnormal = logp_abnormal.reshape(-1).detach().cpu()\n",
    "    \n",
    "    n_idx = int(len(logp_normal) * 0.4)\n",
    "    sorted_indices = torch.sort(logp_normal)[1]\n",
    "    n_idx = sorted_indices[n_idx]\n",
    "    b_n = logp_normal[n_idx] \n",
    "    \n",
    "    sns.distplot(logp_normal,label = 'normal')\n",
    "    sns.distplot(logp_abnormal,label = 'abnormal')\n",
    "    plt.axvline(b_n, color='red', linestyle='--')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# validate_logp_normal(logp2,torch.load(r\"E:\\2023\\NaverProject\\LastCodingProject\\Binary_file\\label_train_flow.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
